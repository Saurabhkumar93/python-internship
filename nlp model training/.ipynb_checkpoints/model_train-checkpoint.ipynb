{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "from spacy.util import filter_spans\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'EXPERIENCE, SMART SEO TECH, Marketing Executive and SEO Consultant, Jan 24 - April 24, Working Platform  Freelancer.com, Fiverr.com, Upwork.com, Responsibilities  Acquire New Projects, Handle Clients, Project &, Work Management, deliver Satisfactory work and reports, Upselling,, Project Restart approach, Bidding on Posted Projects, W3ERA WEB TECHNOLOGY PVT LTD, Marketing Executive, June 2023 - December 2023, Working Platform  Freelancer.com, Fiverr.com, Upwork.com, Responsibilities  Acquire New Projects, Handle Clients, Project &, Work Management, deliver Satisfactory work and reports, Upselling,, Project Restart approach, Bidding on Posted Projects, SEO Executive, Sep. 2021 - June 2023, Responsibilities  Keyword Research, Website Analysis, On-Page, Optimization and Report Preparation, Competitor Analysis, Website, Health Report, Performance Report creation, Regular Monitoring,, Additional Suggestion Report, Backlink submissions, SEO SOLUTION, Off-Page SEO Executive, May 2021 - Sep. 2021, NUTTSBUNNY PRODUCTION, SEO Intern, Oct. 2020 - April 2021', 'entities': [(12, 26, 'ORG'), (28, 47, 'ORG'), (68, 85, 'DATE'), (334, 362, 'ORG'), (385, 410, 'DATE'), (674, 695, 'DATE'), (982, 992, 'DATE'), (1004, 1025, 'ORG'), (1039, 1061, 'DATE')]}\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file = \"train.csv\"  # Change this to the path of your CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Function to get start and end indices of entities in the text\n",
    "def extract_entities(row):\n",
    "    row_num = row['num']\n",
    "    text = row['text']\n",
    "    entities = ast.literal_eval(row['entities'])  # Convert the string representation of list to a real list\n",
    "    entity_list = []\n",
    "    \n",
    "    for entity, label in entities:\n",
    "        start_idx = text.find(entity)\n",
    "        if start_idx != -1:\n",
    "            end_idx = start_idx + len(entity)\n",
    "            entity_list.append((start_idx, end_idx, label))\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'entities': entity_list\n",
    "    }\n",
    "\n",
    "# Apply the function to each row and create a list of dictionaries\n",
    "training_data = df.apply(extract_entities, axis=1).tolist()\n",
    "\n",
    "# Output the result\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 367.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_bin = DocBin()\n",
    "for training_example in tqdm(training_data):\n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in labels: \n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Work Experience, Intern, DAY-NULM - Delhi, Delhi, April 2023 to June 2023, As part of my internship with DAY-NULM( Deendayal Antyodaya Yojana-, National Urban Livelihoods Mission) by Ministry of Housing and Urban, Affairs ( MOHUA) , We created a prototype of an app that would allow, people to donate food and groceries as a way to help the urban poor,, including labourers and vendors, that mobile application aimed at, addressing the nutritional needs of urban poor laborers and street, vendors. In this role, I took charge of frontend development to ensure an, intuitive user interface and seamless user experience., Additionally, I actively contributed to backend development, participating in the design and, implementation of the app s server-side functionalities. Collaborating closely with a multidisciplinary, team, we gained insights into user needs and iterated on app features to better serve the target, demographic. Through user testing and feedback gathering, we facilitated food and grocery donations, to underprivileged communities, promoting social responsibility and community support. Moreover, we, enabled access to affordable meals from community kitchens for users residing in areas where such, facilities were not readily available. And we re contributing to improving the overall health and well-being, of vulnerable populations., Providing access to nutritious food and facilitating food and grocery, donations to underprivileged communities can have a profound impact, on reducing malnutrition rates and promoting better health outcomes, https //nulm. gov. in/', 'entities': [(25, 33, 'ORG'), (36, 41, 'GPE'), (50, 73, 'DATE'), (183, 221, 'ORG'), (224, 229, 'ORG')]}\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file = \"test.csv\"  # Change this to the path of your CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Function to get start and end indices of entities in the text\n",
    "def extract_entities(row):\n",
    "    text = row['text']\n",
    "    entities = ast.literal_eval(row['entities'])  # Convert the string representation of list to a real list\n",
    "    entity_list = []\n",
    "    \n",
    "    for entity, label in entities:\n",
    "        start_idx = text.find(entity)\n",
    "        if start_idx != -1:\n",
    "            end_idx = start_idx + len(entity)\n",
    "            entity_list.append((start_idx, end_idx, label))\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'entities': entity_list\n",
    "    }\n",
    "\n",
    "# Apply the function to each row and create a list of dictionaries\n",
    "testing_data = df.apply(extract_entities, axis=1).tolist()\n",
    "\n",
    "# Output the result\n",
    "print(testing_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]100%|██████████| 15/15 [00:00<00:00, 125.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_bin = DocBin()\n",
    "for training_example in tqdm(testing_data):\n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in labels: \n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"dev.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    370.98    0.00    0.00    0.00    0.00\n",
      "  2     200       1363.04   5283.37   40.66   43.97   37.80    0.41\n",
      "  5     400       1207.68   2367.70   57.99   56.32   59.76    0.58\n",
      "  8     600       1235.36   2137.13   65.38   68.92   62.20    0.65\n",
      " 11     800        466.28   1399.48   73.10   70.22   76.22    0.73\n",
      " 14    1000       1520.80   1248.80   75.88   73.30   78.66    0.76\n",
      " 17    1200        459.43   1071.11   84.24   83.73   84.76    0.84\n",
      " 21    1400        742.65    959.10   86.96   82.87   91.46    0.87\n",
      " 24    1600        289.77    642.53   88.63   84.92   92.68    0.89\n",
      " 28    1800        435.10    627.35   89.70   89.16   90.24    0.90\n",
      " 33    2000        402.70    649.91   92.54   90.64   94.51    0.93\n",
      " 38    2200       3365.33    792.22   94.83   94.55   95.12    0.95\n",
      " 44    2400        457.04    463.14   93.73   91.81   95.73    0.94\n",
      " 50    2600        478.90    455.59   96.93   97.53   96.34    0.97\n",
      " 58    2800        736.88    439.32   97.55   98.15   96.95    0.98\n",
      " 67    3000        411.64    423.72   98.18   97.59   98.78    0.98\n",
      " 77    3200        724.13    498.51   97.87   97.58   98.17    0.98\n",
      " 88    3400        815.43    465.32   98.17   98.17   98.17    0.98\n",
      " 98    3600        680.07    462.29   98.77   99.38   98.17    0.99\n",
      "109    3800        755.03    418.75   98.15   99.38   96.95    0.98\n",
      "119    4000        771.43    395.17   97.89   97.01   98.78    0.98\n",
      "130    4200        427.61    346.00   96.93   97.53   96.34    0.97\n",
      "140    4400        591.01    359.80   98.47   98.77   98.17    0.98\n",
      "150    4600       1404.98    469.81   98.17   98.17   98.17    0.98\n",
      "161    4800        871.63    358.11   98.19   97.02   99.39    0.98\n",
      "172    5000        692.61    345.33   97.87   97.58   98.17    0.98\n",
      "182    5200        893.04    338.93   98.77  100.00   97.56    0.99\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
